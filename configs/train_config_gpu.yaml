# GPU-optimized training configuration for L1
# STABLE VERSION - Conservative settings to prevent PC freezing

# Model configuration - intelligent but stable
model:
  vocab_size: 32003         # Actual BPE tokenizer size (32000 + 3 special tokens)
  max_seq_length: 512       # Conservative for stability (was 1024)
  n_layers: 12              # Reduced from 16 for stability
  n_heads: 12               # Reduced from 16 for stability
  n_embd: 768               # Reduced from 1024 for stability
  n_inner: 3072             # 4x embedding size
  dropout: 0.1
  layer_norm_epsilon: 0.00001
  initializer_range: 0.02
  use_cache: true

# Training configuration - stable settings
training:
  output_dir: "models/l1-gpu-compatible"
  num_epochs: 10            # Reduced from 15
  batch_size: 4             # Reduced from 6 for stability
  learning_rate: 0.0001     # Slightly higher for faster convergence
  weight_decay: 0.01
  max_grad_norm: 1.0
  save_steps: 500
  logging_steps: 50
  max_steps: null           
  
  # Optimized checkpoint frequency for ultra-fast training
  checkpoint_every_steps: 1000  # Every ~2 minutes instead of 10 seconds
  max_checkpoints_to_keep: 5
  warmup_steps: 500         # Reduced warmup
  lr_scheduler: "linear"    # More stable than cosine
  mixed_precision: true     # Keep AMP for efficiency
  gradient_accumulation_steps: 4
  dataloader_num_workers: 2 # Reduced from 4

# Data configuration - conservative
data:
  train_data_path: "data/processed/train.txt"
  tokenizer_path: "data/processed/tokenizer.json"  # BPE tokenizer from prepare_large_dataset.py
  max_length: 512           # Match model max_seq_length
  
# Performance optimizations - minimal
performance:
  compile_model: false      # Disable compilation that might cause issues
  gradient_checkpointing: true
  use_torch_compile: false  # Disable advanced compilation
