# GPU-optimized training configuration for L1
# STABLE VERSION - Conservative settings to prevent PC freezing

# Model configuration - V2.0 with advanced features
model:
  vocab_size: 32000         # BPE tokenizer vocabulary size (overridden by actual tokenizer size during training)
  max_seq_length: 2048      # Extended from 512 with RoPE support (4096 available)
  n_layers: 12              # Stable depth
  n_heads: 12               # Query heads
  n_embd: 768               # Embedding dimension
  n_inner: 3072             # 4x embedding size (feed-forward)
  dropout: 0.1
  layer_norm_epsilon: 0.00001
  initializer_range: 0.02
  use_cache: true
  
  # V2.0 Advanced Attention Features
  use_flash_attention: true  # 2-4x speedup with Flash Attention 2
  use_rope: true             # Rotary Position Embeddings for better long-context
  use_gqa: true              # Grouped Query Attention for efficient inference
  n_kv_heads: 4              # GQA: 4 KV heads for 12 Q heads (3:1 ratio, saves memory)
  rope_theta: 10000.0        # RoPE base frequency
  bias: true                 # Use bias in linear layers

# Training configuration - stable settings
training:
  output_dir: "models/l1-gpu-compatible"
  num_epochs: 10            # Reduced from 15
  batch_size: 4             # Reduced from 6 for stability
  learning_rate: 0.0001     # Slightly higher for faster convergence
  weight_decay: 0.01
  max_grad_norm: 1.0
  save_steps: 500
  logging_steps: 50
  max_steps: null           
  
  # Optimized checkpoint frequency for ultra-fast training
  checkpoint_every_steps: 1000  # Every ~2 minutes instead of 10 seconds
  max_checkpoints_to_keep: 5
  warmup_steps: 500         # Reduced warmup
  lr_scheduler: "linear"    # More stable than cosine
  mixed_precision: true     # Keep AMP for efficiency
  gradient_accumulation_steps: 4
  dataloader_num_workers: 2 # Reduced from 4

# Data configuration - conservative
data:
  train_data_path: "data/processed/train.txt"
  tokenizer_path: "data/processed/tokenizer.json"  # BPE tokenizer from prepare_large_dataset.py
  max_length: 512           # Match model max_seq_length
  
# Performance optimizations - minimal
performance:
  compile_model: false      # Disable compilation that might cause issues
  gradient_checkpointing: true
  use_torch_compile: false  # Disable advanced compilation
