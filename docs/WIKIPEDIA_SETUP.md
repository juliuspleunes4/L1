# ğŸŒ Wikipedia Dataset Setup for L1

## âœ… Current Status

**Your L1 is already set up with Wikipedia Simple English dataset!**
- âœ… **90,000+ samples** processed and ready
- âœ… **High-quality educational content** 
- âœ… **BPE tokenizer** with 1,783 vocabulary trained
- âœ… **Training data** ready in `data/processed/`

## ğŸ“Š Current Dataset Details

**Wikipedia Simple English Dataset:**
- **Source**: High-quality Wikipedia articles in simplified English
- **Content**: Educational articles covering diverse topics
- **Quality**: â­â­â­â­â­ Very High - structured, factual content
- **Language**: Simplified English (perfect for training)
- **Status**: âœ… **Ready for training**

### Current Files
```bash
# Your processed data is ready:
data/processed/train.txt        # 90,247 training samples
data/processed/val.txt          # 10,027 validation samples  
data/processed/tokenizer.json   # BPE tokenizer (1,783 vocab)
```

## ğŸš€ Ready to Train

Since your Wikipedia dataset is already prepared, you can immediately start training:

```bash
# Start GPU training (recommended)
python train_gpu_compatible.py

# Or CPU training for testing
python train_minimal.py
```

## ğŸ”„ Alternative Wikipedia Options

If you want more Wikipedia data in the future:

### Option 1: Use Full Wikipedia
```bash
# Much larger dataset (500k+ samples)
python add_dataset.py wikipedia_full
```

### Option 2: Combine with Other Sources
```bash
# Add more educational content
python add_dataset.py --preset knowledge   # Wikipedia + Papers + Books
```

## ğŸ’¡ Why Wikipedia Simple is Excellent

Your current Wikipedia Simple dataset provides:

### Content Quality
- **Factual Information**: Real-world knowledge and facts
- **Educational Content**: Covering science, history, culture, technology
- **Consistent Style**: Encyclopedia writing style  
- **Simplified Language**: Easier for models to learn patterns
- **Broad Vocabulary**: Diverse topics = rich vocabulary

### Training Benefits
- **High-Quality Output**: Your model will generate factual, well-structured text
- **Diverse Knowledge**: Can discuss many topics intelligently
- **Good Grammar**: Learns proper English grammar and style
- **Reliable Content**: No spam, low-quality, or inappropriate content

### Perfect for Your GPU Setup
- **Right Size**: 90k samples ideal for RTX 5060 Ti development
- **Memory Efficient**: Fits well within 16GB VRAM constraints
- **Training Time**: Reasonable training duration (6-12 hours for good results)

## ğŸ“ˆ Expected Training Results

With your Wikipedia dataset, expect your L1 model to:
- âœ… **Generate factual content** about various topics
- âœ… **Use proper grammar** and sentence structure
- âœ… **Display broad knowledge** across multiple domains
- âœ… **Maintain consistent style** in generated text
- âœ… **Avoid hallucinations** common with lower-quality datasets

## ğŸ” Monitoring Your Training

```bash
# Watch training progress
tail -f models/l1-gpu-compatible/training.log

# Check GPU utilization
nvidia-smi

# View model outputs during training
python generate_simple.py --prompt "Wikipedia is"
```

## ï¿½ Dataset Management

### Check Current Status
```bash
# Verify your data is ready
ls -la data/processed/
# Should show: train.txt, val.txt, tokenizer.json

# View dataset statistics
python -c "
with open('data/processed/train.txt', 'r', encoding='utf-8') as f:
    lines = f.readlines()
    print(f'Training samples: {len(lines):,}')
    print('Sample text:', lines[0][:100] + '...')
    print('âœ… Wikipedia dataset ready!')
"
```

### Start Training
```bash
# Begin or resume GPU training
python train_gpu_compatible.py

# Expected output:
# ğŸ“ Training Configuration:
#    â”œâ”€â”€ Dataset: Wikipedia Simple (90,247 samples)
#    â”œâ”€â”€ Vocabulary: 1,783 tokens
#    â”œâ”€â”€ Model: 155.8M parameters
#    â”œâ”€â”€ GPU: RTX 5060 Ti (16GB)
#    â””â”€â”€ Checkpoints: Every 100 steps
```

### Generate Text
```bash
# Test your trained model
python generate_simple.py \
    --prompt "The history of artificial intelligence" \
    --model_path models/l1-gpu-compatible

# Expected quality: Educational, factual responses about AI history
```

## ğŸš¨ Troubleshooting

### If Training Fails to Start
```bash
# Check data files exist
ls data/processed/

# If missing, regenerate:
python prepare_large_dataset.py
```

### Memory Issues
```bash
# Reduce batch size in configs/train_config_gpu.yaml:
# batch_size: 4  # Instead of 8
```

### Generation Issues
```bash
# Check model checkpoint exists
ls models/l1-gpu-compatible/

# Use latest checkpoint
python generate_simple.py \
    --prompt "Test" \
    --model_path models/l1-gpu-compatible/latest_checkpoint.pt
```

## âœ… Summary

Your L1 is perfectly set up with Wikipedia Simple:
- âœ… **High-quality dataset** (90k+ educational samples)
- âœ… **Ready for training** (preprocessed and tokenized)  
- âœ… **Optimal size** (perfect for RTX 5060 Ti development)
- âœ… **Production-ready** (factual, well-structured content)

**Next Step**: Start training with `python train_gpu_compatible.py`

Your Wikipedia dataset will produce a knowledgeable, factual LLM! ğŸ§ ğŸš€
