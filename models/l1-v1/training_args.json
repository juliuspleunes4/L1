{
  "model": {
    "vocab_size": 1000,
    "max_seq_length": 512,
    "n_layers": 6,
    "n_heads": 8,
    "n_embd": 512,
    "n_inner": 2048,
    "dropout": 0.1,
    "layer_norm_epsilon": 1e-05,
    "initializer_range": 0.02,
    "use_cache": true,
    "pad_token_id": 0,
    "eos_token_id": 3,
    "bos_token_id": 2
  },
  "training": {
    "num_epochs": 5,
    "batch_size": 4,
    "eval_batch_size": 4,
    "learning_rate": 0.0003,
    "weight_decay": 0.01,
    "max_grad_norm": 1.0,
    "optimizer_type": "adamw",
    "beta1": 0.9,
    "beta2": 0.999,
    "eps": "1e-8",
    "scheduler_type": "cosine",
    "warmup_steps": 100,
    "warmup_ratio": null,
    "max_seq_length": 512,
    "dataloader_num_workers": 0,
    "dataloader_pin_memory": false,
    "eval_steps": 100,
    "save_steps": 200,
    "logging_steps": 10,
    "max_steps": 1000,
    "output_dir": "./models/l1-v1",
    "save_total_limit": 3,
    "load_best_model_at_end": true,
    "eval_strategy": "steps",
    "metric_for_best_model": "eval_loss",
    "greater_is_better": false,
    "device": "auto",
    "fp16": false,
    "gradient_checkpointing": false,
    "wandb_project": "l1-llm",
    "wandb_run_name": "l1-v1-training",
    "report_to": [
      "tensorboard"
    ],
    "seed": 42
  },
  "data": {
    "train_data_path": "./data/processed/train.txt",
    "val_data_path": "./data/processed/val.txt",
    "test_data_path": "./data/processed/test.txt",
    "val_split": 0.1,
    "test_split": 0.1,
    "tokenizer_type": "bpe",
    "vocab_size": 1000,
    "max_length": 512,
    "stride": 256
  },
  "generation": {
    "max_new_tokens": 50,
    "temperature": 0.8,
    "top_k": 40,
    "top_p": 0.9,
    "do_sample": true
  }
}